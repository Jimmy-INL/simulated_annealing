{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-13T05:17:53.469471Z",
     "start_time": "2018-05-13T05:17:53.464947Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from random import random\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import fbeta_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-13T05:12:49.035664Z",
     "start_time": "2018-05-13T05:12:49.032022Z"
    }
   },
   "outputs": [],
   "source": [
    "# Parameters that are kept constant during the tuning process\n",
    "param = {'silent':1,\n",
    "         'min_child_weight':1,\n",
    "         'objective':'binary:logistic',\n",
    "         'eval_metric':'auc',\n",
    "         'seed': 42}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-13T05:12:47.489287Z",
     "start_time": "2018-05-13T05:12:47.481634Z"
    }
   },
   "outputs": [],
   "source": [
    "# Parameter search space\n",
    "tune_dic = OrderedDict()\n",
    "tune_dic['max_depth']= [5,10,15,20,25] ## maximum tree depth\n",
    "tune_dic['subsample']=[0.5,0.6,0.7,0.8,0.9,1.0] ## proportion of training instances used in trees\n",
    "tune_dic['colsample_bytree']= [0.5,0.6,0.7,0.8,0.9,1.0] ## subsample ratio of columns\n",
    "tune_dic['eta']= [0.01,0.05,0.10,0.20,0.30,0.40]  ## learning rate\n",
    "tune_dic['gamma']= [0.00,0.05,0.10,0.15,0.20]  ## minimum loss function reduction required for a split\n",
    "tune_dic['scale_pos_weight']=[1,2,5,8,10,15,20] ## relative weight of positive/negative instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-13T05:13:44.957914Z",
     "start_time": "2018-05-13T05:13:44.946267Z"
    }
   },
   "outputs": [],
   "source": [
    "# Custom metric calculation function\n",
    "def f2_score(y_pred, y_true): return fbeta_score(y_true, (y_pred>=0.5).astype(int), beta=2)\n",
    "\n",
    "# Function to train model\n",
    "def train_model(curr_params, param, Xtrain, Xvalid, num_rounds=20):\n",
    "    \"\"\"\n",
    "    Train the model with given set of hyperparameters\n",
    "    curr_params - Dict of hyperparameters and chosen values\n",
    "    param - Dict of hyperparameters that are not tuned\n",
    "    Xtrain - DMatrix of traing data\n",
    "    Ytrain - Training labels\n",
    "    Ytrain - DMatrix of validation data\n",
    "    Yvalid - Validaion labels\n",
    "    \"\"\"\n",
    "    param.update(curr_params)\n",
    "    model = xgb.train(param, Xtrain, num_boost_round=num_rounds)\n",
    "    preds = model.predict(Xvalid)\n",
    "    labels = Xvalid.get_label()\n",
    "    f_score = f2_score(preds, labels)\n",
    "    \n",
    "    return model, f_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-13T05:13:54.382125Z",
     "start_time": "2018-05-13T05:13:54.362268Z"
    }
   },
   "outputs": [],
   "source": [
    "def choose_params(tune_dic, curr_params=None):\n",
    "    \"\"\"\n",
    "    Function to choose parameters for next iteration, given current parameters\n",
    "    tune_dic - Dict of Hyperparameter search space\n",
    "    curr_params - Dict of current hyperparameters\n",
    "    \"\"\"\n",
    "    if curr_params:\n",
    "        next_params = curr_params.copy()\n",
    "        param_to_update = np.random.choice(list(tune_dic.keys()))\n",
    "        param_vals = tune_dic[param_to_update]\n",
    "        curr_index = param_vals.index(curr_params[param_to_update])\n",
    "        if curr_index == 0:\n",
    "            next_params[param_to_update] = param_vals[1]\n",
    "        elif curr_index == len(param_vals) - 1:\n",
    "            next_params[param_to_update] = param_vals[curr_index - 1]\n",
    "        else:\n",
    "            next_params[param_to_update] = \\\n",
    "                param_vals[curr_index + np.random.choice([-1,1])]\n",
    "    else:\n",
    "        next_params = dict()\n",
    "        for k, v in tune_dic.items():\n",
    "            next_params[k] = np.random.choice(v)\n",
    "\n",
    "    return next_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-13T05:18:09.318118Z",
     "start_time": "2018-05-13T05:18:09.201241Z"
    }
   },
   "outputs": [],
   "source": [
    "def simulate_annealing(fn_train, tune_dic, X_train, X_valid, Y_train=None,\n",
    "                       Y_valid=None, maxiters=100, alpha=0.85, beta=1.3,\n",
    "                       T=0.40, update_iters=5):\n",
    "    \"\"\"\n",
    "    Function to perform hyperparameter search using simulated annealing\n",
    "    fn_train - Function to train the model (Should return model and metric value)\n",
    "    tune_dic - Dictionary of Hyperparameter search space\n",
    "    maxiters - Number of iterations to perform the parameter search\n",
    "    alpha - factor to reduce temperature\n",
    "    beta - constant in probability estimate\n",
    "    T - Initial temperature\n",
    "    update_iters - # of iterations required to update temperature\n",
    "    \"\"\"\n",
    "    columns = [*tune_dic.keys()] + ['Metric', 'Best Metric']\n",
    "    results = pd.DataFrame(index=range(maxiters), columns=columns)\n",
    "    best_metric = -1.\n",
    "    prev_metric = -1.\n",
    "    prev_params = None\n",
    "    best_params = dict()\n",
    "    weights = list(map(lambda x: 10**x, list(range(len(tune_dic)))))\n",
    "    hash_values = set()\n",
    "    \n",
    "    for i in range(maxiters):\n",
    "        print('Starting Iteration {}'.format(i))\n",
    "        while True:\n",
    "            curr_params = choose_params(tune_dic, prev_params)\n",
    "            indices = [tune_dic[k].index(v) for k, v in curr_params.items()]\n",
    "            hash_val = sum([i * j for (i, j) in zip(weights, indices)])\n",
    "            if hash_val in hash_values:\n",
    "                print('Combination revisited')\n",
    "            else:\n",
    "                hash_values.add(hash_val)\n",
    "                break\n",
    "        \n",
    "        model, metric = fn_train(\n",
    "            curr_params,\n",
    "            param,\n",
    "            X_train,\n",
    "            X_valid,\n",
    "            num_rounds=num_rounds)\n",
    "\n",
    "        if metric > prev_metric:\n",
    "            print('Local Improvement from {:8.4f} to {:8.4f} - parameters accepted'\\\n",
    "                  .format(prev_metric, metric))\n",
    "            prev_params = curr_params.copy()\n",
    "            prev_metric = metric\n",
    "\n",
    "            if fscore > best_fscore:\n",
    "                print('Global improvement from {:8.4f} to {:8.4f} - best parameters updated'\\\n",
    "                  .format(best_metric, metric))\n",
    "                best_metric = metric\n",
    "                best_params = curr_params.copy()\n",
    "        else:\n",
    "            rnd = np.random.uniform()\n",
    "            diff = metric - prev_metric\n",
    "            threshold = np.exp(beta * diff / T)\n",
    "            if rnd < threshold:\n",
    "                print(\"\"\"No Improvement but parameters accepted. F-Score change: {:8.4f} \n",
    "                threshold: {:6.4f} random number: {:6.4f}\n",
    "                \"\"\".format(diff, threshold, rnd))\n",
    "                prev_metric = metric\n",
    "                prev_params = curr_params\n",
    "            else:\n",
    "                print(\"\"\"No Improvement and parameters rejected. F-Score change: {:8.4f} \n",
    "                threshold: {:6.4f} random number: {:6.4f}\n",
    "                \"\"\".format(diff, threshold, rnd))\n",
    "\n",
    "        results.loc[i, list(curr_params.keys())] = list(curr_params.values())\n",
    "        results.loc[i, 'F-Score'] = metric\n",
    "        results.loc[i, 'Best F-Score'] = best_metric\n",
    "\n",
    "        if i % update_iters == 0: T = alpha * T\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
